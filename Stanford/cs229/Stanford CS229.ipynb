{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969b181a-4ec4-45b4-a7cb-b155f022c2d0",
   "metadata": {},
   "source": [
    "# CS229 MACHINE LEARNING Course, Autumn 2018 from [Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8949d6-4df4-460c-ae8a-14ecbd6a23b1",
   "metadata": {},
   "source": [
    "Also see the [Coursera course](https://www.youtube.com/playlist?list=PLxfEOJXRm7eZKJyovNH-lE3ooXTsOCvfC) from 2022\n",
    "\n",
    "Listen to the first lecture in Andrew Ng's machine learning course. This course provides a broad introduction to machine learning and statistical pattern recognition. Learn about both supervised and unsupervised learning as well as learning theory, reinforcement learning and control. Explore recent applications of machine learning and design and develop algorithms for machines.\n",
    "\n",
    "Andrew Ng is an Adjunct Professor of Computer Science at Stanford University. View more about Andrew on his website: https://www.andrewng.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e63a3-bc4b-4afd-b3fa-b62002b87179",
   "metadata": {},
   "source": [
    "# Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe1f9b-bbc6-49d9-8db9-45c48114bfde",
   "metadata": {},
   "source": [
    "Contents:\n",
    "- machine learning definition\n",
    "- learning theory\n",
    "- deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736432ad-9183-4831-8019-3d32f3715738",
   "metadata": {},
   "source": [
    "## 1.1 Definitions\n",
    "\n",
    "'Field of study that gives computers the ability to learn without being explicitly programmed' Arthur Samuel (1959)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6dba4-da8c-4819-8f72-1f0a98fabd81",
   "metadata": {},
   "source": [
    "## 1.2 Supervised learning\n",
    "\n",
    "99% of market capitalization of ML software is made with the supervised learning algorithms.\n",
    "\n",
    "They are:\n",
    "- regression \n",
    "- classification/categorization\n",
    "\n",
    "We are mapping the features `X` to target `y` (labeled data).\n",
    "\n",
    "$$\n",
    "X \\to y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf322cf-5ad8-4db5-a87f-fd8de7712559",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "We get a **numbers** from infinite number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c17e21-cc1a-4406-8808-cf9c5e0cfc5e",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We get a **small number** of categories from infinite number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87cd8b-2c49-4945-a88a-a80c5050ee68",
   "metadata": {},
   "source": [
    "## 1.3 ML strategy\n",
    "\n",
    "Conveying more systematic engineering principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ec1c0-ad17-4d37-9aed-a8ea410a58d5",
   "metadata": {},
   "source": [
    "### Speeding the algorithm up\n",
    "\n",
    "Less experienced software engineer will dive in and optimize the code to make it run faster.\n",
    "\n",
    "More experienced engineer will run a profiler to try to figure out what part of code is actually a bottleneck and then just focus on changing that.\n",
    "\n",
    "Andrew Ng's [book](https://www.mlyearning.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afce178-0558-4079-8303-f9fff04db626",
   "metadata": {},
   "source": [
    "## 1.4 Deep learning\n",
    "\n",
    "CS230"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7702bbb-ae1b-4c71-bb86-1113cf43814d",
   "metadata": {},
   "source": [
    "## 1.5 Unsupervised learning\n",
    "\n",
    "No labels, there is only features `X` without given target `y`. so the algorithm is looking for interesting structure in the data - **clustering** the data:\n",
    "\n",
    "- KMeans algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee475925-37e8-492b-abd8-b4dc7587c360",
   "metadata": {},
   "source": [
    "### Cocktail Party Problem\n",
    "\n",
    "How to extract different voices from the voice recording in the crowded room?\n",
    "\n",
    "- ICA algorithm - Independent Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea293a-407e-4725-9a20-8d1254b93d33",
   "metadata": {},
   "source": [
    "## 1.6 Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabd9b9-559a-434b-8be6-5dde1be6823a",
   "metadata": {},
   "source": [
    "Dressing off the algorithm with the system of positive and negative rewards (reinforcement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c66721-8e16-4004-8faf-fd8baa1fdbc5",
   "metadata": {},
   "source": [
    "# Lecture 2 | Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45eb793-71b5-4c2a-8988-5e1567576d42",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1635df-de70-41ab-991d-f78278b9d440",
   "metadata": {},
   "source": [
    "### The idea\n",
    "\n",
    "One of the simplest supervised learning regression problem.\n",
    "\n",
    "We have:\n",
    "\n",
    "`training set` -> `learning algorithm` -> `hypothesis H`\n",
    "\n",
    "The goal:\n",
    "\n",
    "- we `input` some info into `H` to get the `output`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc50744-2318-4fa6-8747-9cae6d184d17",
   "metadata": {},
   "source": [
    "### How to represent the `H`?\n",
    "\n",
    "It is an affine function:\n",
    "\n",
    "$$\n",
    "h(X) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ...\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(X) = \\sum_{j = 0}^n \\theta_j \\cdot x_j\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_0$ = 1, $\\Theta = \n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   \\theta_0\\\\\n",
    "   \\theta_1\\\\\n",
    "   \\cdots \\\\\n",
    "   \\theta_n\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$, $X = \n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   x_0\\\\\n",
    "   x_1\\\\\n",
    "   \\cdots \\\\\n",
    "   x_n\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$, and `n` is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d17d1d-c825-4581-a139-fa8b8fc1c01d",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "$\\Theta$ - **parameters** of the learning algorithm.\n",
    "\n",
    "> The job of the algorithm is to choose parameters $\\theta$ that will make good predictions.\n",
    "\n",
    "`m` - training examples (the number of the table's lines)\n",
    "\n",
    "`X` - **features** ('inputs'/'input attributes')\n",
    "\n",
    "`y` - **target variable** ('output')\n",
    "\n",
    "`(x, y)` - training example\n",
    "\n",
    "$(x^{(i)}, y^{(i)})$ - $i^{th}$ training example\n",
    "\n",
    "`n` - the number of features (+1 extra ('dummy') feature $x_0 = 1$, so we have `n+1` dimensional features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660feada-4a67-4ca5-91d6-aa7c2a3b8461",
   "metadata": {},
   "source": [
    "### How do we choose parameters $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda557db-809c-4a61-82c7-7f4537d6667f",
   "metadata": {},
   "source": [
    "Choose $\\theta$ such that $h(x) \\approx y$ for training examples:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = h(x)\n",
    "$$\n",
    "\n",
    "The linear regression is also called **ordinary least squares**:\n",
    "\n",
    "$$\n",
    "\\left(h_{\\theta}(x) - y\\right)^2\n",
    "$$\n",
    "\n",
    "We need to choose values for $\\theta$ to minimize ordinary least squares:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\cdot \\sum_{i=1}^m \\left(h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "`1/2` is taken as a convention to simplify the math when we take derivatives to minimize $J(\\theta)$ later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d551ba6-2147-4457-80aa-594865684f50",
   "metadata": {},
   "source": [
    "## Gradient Descent | Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab013b4-d500-4c22-a53e-8e8d4f7c621b",
   "metadata": {},
   "source": [
    "is used to minimize the cost function $J(\\theta)$.\n",
    "\n",
    "1. We start from some value of $\\theta$ (say, $\\theta = \\overrightarrow{0}$)\n",
    "\n",
    "2. Keep changing $\\theta$ to reduce $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992d04b-8def-46a2-91eb-272e78217a37",
   "metadata": {},
   "source": [
    "<div><img src=\"https://miro.medium.com/proxy/1*f9a162GhpMbiTVTAua_lLQ.png\" alt=\"Simple decision tree\" style=\"width: 600px; margin-left: 0%\"><div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc39c1-9c7f-4c8a-9128-29175360a444",
   "metadata": {},
   "source": [
    "But starting from the different points you will finally get to different local minimum, so\n",
    "\n",
    "> there will not be local optimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d326d-3fb0-4801-bbe0-e35bce55139b",
   "metadata": {},
   "source": [
    "### Algorithm formalization\n",
    "\n",
    "Let's formalize the algorithm for one training example.\n",
    "\n",
    "The `m` and the $J(\\theta)$ are fixed so we modify only parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac {\\delta}{\\delta \\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "- `:=` - assignment (like n += 1 with each iteration)\n",
    "- $\\alpha$ - learning rate (in practice $\\alpha = 0.01$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d63cb-ef77-4a68-8bf2-e11fc36383b8",
   "metadata": {},
   "source": [
    "#### Partial derivative\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac {\\delta}{\\delta \\theta_j} J(\\theta_j) & = \\frac {\\delta}{\\delta \\theta_j} \\frac{1}{2} \\left(h_{\\theta}(x) - y\\right)^2 = \\\\\n",
    "& = 2 \\frac {1}{2} (h_{\\theta}(x) - y) \\cdot \\frac {\\delta}{\\delta \\theta_j}(h_{\\theta}(x) - y) = \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot \\frac {\\delta}{\\delta \\theta_j} (\\theta_0 x_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n -y) \\\\\n",
    "& = (h_{\\theta}(x) - y) \\cdot x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fea194-7ace-48c6-a683-5a8b441cce1e",
   "metadata": {},
   "source": [
    "### General formula\n",
    "\n",
    "So the updated formula for all the training examples will be:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_j & := \\theta_j - \\alpha \\cdot \\sum_{i=i}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} = \\\\\n",
    "& = \\theta_j - \\frac {\\delta}{\\delta \\theta_j} J(\\theta) \n",
    "\\end{align*}\n",
    "\n",
    "for $j = 0, 1, \\cdots, n$\n",
    "\n",
    "Repeat until convergence.\n",
    "\n",
    "The plot will always look like a bowl (ellipses) because we square the function, so there are no local optima, there is one global optimum:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c7c11-dac7-499a-8ffb-3b88f9a80eea",
   "metadata": {},
   "source": [
    "<div><img src=\"https://i1.wp.com/knowhowspot.com/wp-content/uploads/2019/04/IMG_20190410_002014.jpg?fit=1024%2C707&ssl=1\" alt=\"gradient descent plot\" style=\"width: 500px; margin-left: 0%\"><div>\n",
    "    \n",
    "<div><img src=\"https://miro.medium.com/proxy/1*1GDN-VD4BC2EBW1jHbTdCg.png\" alt=\"gradient descent contours\" style=\"width: 500px; margin-left: 0%\"><div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404a03b-136c-4b3d-8a8f-79f30832cdec",
   "metadata": {},
   "source": [
    "The steepest descent is at 90 degrees to the contours. So, eventually the algorithm converges to the global minimum.\n",
    "\n",
    "If you take to large learning rate `alpha` you can run past the minimum. If you set it too small you will need a lot if iterations which slows down the algorithm. So this is a matter of experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528784c-2942-47a7-92df-66291f0a9ec4",
   "metadata": {},
   "source": [
    "### 'Batch gradient descent' name\n",
    "\n",
    "**Batch** means you look at the entire dataset as one batch of data and you're going to process all the data as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46985d0-ab1c-4c7f-bf0a-5e156345568a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ac8b7-d0a8-4b84-9ca4-a75033614096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdcee9-a531-449e-bf05-c3d674a34cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0e242-efdd-45b8-b84e-c3f69e673b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d383065-0944-4f3b-b121-7a6f13a68a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
