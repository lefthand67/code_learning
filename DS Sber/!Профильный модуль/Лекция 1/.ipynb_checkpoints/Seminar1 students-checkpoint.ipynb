{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbfDCKAhLZl5"
   },
   "source": [
    "# Семинар 1\n",
    "## Первичный анализ данных  и EDA (Exploratory Data Analysis)\n",
    "\n",
    "### План занятия:\n",
    "1. Что такое EDA и из чего он состоит \n",
    "2. Практика на реальном датасете:\n",
    "    - Загрузка датасета\n",
    "    - Первичный анализ, очистка\n",
    "    - Визуализация фичей и поиск зависимостей\n",
    "    - Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgRRQ8GSLaeJ"
   },
   "source": [
    "## **EDA** - разведочный анализ данных\n",
    "\n",
    "Место EDA в процессе анализа данных:  \n",
    "\n",
    "![eda.png](https://waksoft.susu.ru/wp-content/uploads/2021/07/eda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfL1hjhgLnAR"
   },
   "source": [
    "### Основные этапы анализа данных:\n",
    "- Извлечение данных\n",
    "- Подготовка данных — очистка данных\n",
    "- Подготовка данных — преобразование данных\n",
    "- Исследование и визуализация данных\n",
    "\n",
    "И только потом - предсказательная модель.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHDK2yhBLahX"
   },
   "source": [
    "# Перейдем к практике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nms2vLoY-6zm"
   },
   "source": [
    "# Постановка задачи\n",
    "Постановка задачи и оценка имеющихся данных — первый шаг на пути к решению. Cделать это нужно ещё до того, как будет написана первая строчка кода.\n",
    "\n",
    "Наши данные — открытые сведения о [энергопотреблении зданий в Нью-Йорке](https://www1.nyc.gov/html/gbee/html/plan/ll84_scores.shtml).\n",
    "\n",
    "Наша цель — предсказать рейтинг энергопотребления (Energy Star Score) здания и понять, какие признаки оказывают на него сильнейшее влияние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn_re_Y3-6zs"
   },
   "source": [
    "## Чистка данных\n",
    "Вопреки тому, какое впечатление может сложиться после посещения различных курсов и чтения статей по машинному обучению, данные не всегда представляют собой идеально организованный набор наблюдений без каких-либо пропусков или аномалий (например, можно взглянуть на известные наборы данных mtcars и  iris). Обычно данные содержат в себе кучу мусора, который необходимо почистить, да и вообще сами данные порой лучше воспринимать критически, для того чтобы затем привести их в приемлемый формат. Чистка данных — это необходимый этап решения почти любой реальной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk6zBYAOPE5h"
   },
   "source": [
    "### Импортируем необходимые библиотеки и изменим настроки отображения библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrd94n9V-6zt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RfUF_haO0jo"
   },
   "outputs": [],
   "source": [
    "# Это не обязательно, но позволяет красиво отображать информацию в блокноте\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "sns.set(font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1xzw9bJOypy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCj3O9Ge-6zw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Считаем данные и посмотрим на первые строки \n",
    "# Ссылка на данные https://raw.githubusercontent.com/ddvika/DS_2021/main/lecture_2/Energy_consumption_NY.csv\n",
    "\n",
    "data = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XKtjHd5BtJW"
   },
   "outputs": [],
   "source": [
    "print(\"Размеры датасета: \", ...)\n",
    "print(\"Колонки датасета: \", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T3RLRy_BjQH"
   },
   "outputs": [],
   "source": [
    "# Воспользуемся методом info для того, чтобы получить представление о стобцах датафрейма\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY4Ifvui-6zx"
   },
   "source": [
    "Это только фрагмент данных, весь набор содержит 60 признаков. \n",
    "\n",
    "Но уже заметна пара проблем: \n",
    "1. Мы уже знаем, что хотим предсказать ENERGY STAR Score, но хорошо бы понять, что из себя представляют остальные признаки. Это не всегда проблема, иногда удается решить задачу машинного обучения, не имея почти никакого представления о том, что признаки из себя представляют. Но для нас важна интерпретируемость, поэтому важно понимать, что несут в себе основные признаки.\n",
    "2. Не все признаки для нас одинаково важны, но с нашим целевым признаком точно нужно разобраться. (А он представляет собой «Оценку в баллах от 1 до 100 основанную на предоставленных сведениях о потреблении электроэнергии. Рейтинг энергопотребления это относительная величина, используемая для сравнения эффективности использования энергии различными зданиями.»)\n",
    "3. Пропущенные данные, вставленные в набор, выглядят как строка с записью “Not Available”. Это означает, что Python, даже если эта колонка содержит в себе преимущественно числовые признаки, будет интерпретировать её как тип данных object, потому что Pandas интерпретируют любой признак содержащий строковые значения как строку.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwUWCHxbgg-y"
   },
   "source": [
    "---\n",
    "\n",
    "## **Отступление**\n",
    "### Немного об интерпретируемости\n",
    "\n",
    "**Интерпретируемость** — это свойство модели, которое показывает, что структуру данной модели может объяснить человек. При этом структура модели не противоречит данным, на которых данная модель построена, а также она сохраняет некоторые свойства предоставленных данных. При интерпретации модели могут быть объяснены принципы и закономерности, которые использует сама модель для предсказания меток класса на конкретных данных.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Alexandru-Bogdan-Georgescu/publication/344893857/figure/fig6/AS:951077046325248@1603765846204/SHAP-force-plot-of-predictions-from-Classifier-T-for-the-MIT-compounds-LuNiO3-and-NdNiO3.ppm)\n",
    "\n",
    "Пример shap-plot, который объясняет предсказание модели: классификатор, который определяет одно из двух соединений -- LuNiO$_3$ или NdNiO$_3$.\n",
    "\n",
    "#### **Конец отступления**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biUTXWG1-6zx"
   },
   "source": [
    "# Data Types and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwZAH7Rm-6zy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Еще раз воспользуемся методом dataframe.info\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RwCD9RTNXzv"
   },
   "outputs": [],
   "source": [
    "# Посчитаем сколько в датасете пропусков, воспользовавшись методом isna\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fia1feTC-6zz"
   },
   "source": [
    "Очевидно, многие признаки, являющиеся изначально числовыми (например, площади), интерпретированы как object. Анализировать их крайне сложно, так что сначала конвертируем их в числа, а именно в тип float.\n",
    "\n",
    "### Конвертируем данные в подходящий формат\n",
    "\n",
    "Заменим значение “Not Available” в данных на «не число» ( np.nan — «not a number»), которое Python все же интерпретирует как число. Это позволит изменить тип соответствующих числовых признаков на float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7g6t8aF-6zz"
   },
   "outputs": [],
   "source": [
    "# Для замены значений в датафрейме воспользуемся методом replace({from_val1: to_val1, ...})\n",
    "\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7m0rp7oAYLmm"
   },
   "outputs": [],
   "source": [
    "# Найдем колонки, которые должны быть числовыми выведя несколько строчек датафрейма\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BZcMg1zuI3v"
   },
   "outputs": [],
   "source": [
    "# Найдем какие подстроки должны содержаться в названиях таких столбцов\n",
    "\n",
    "part_name_numeric_cols = ['ft²', 'kBtu', 'Metric Tons CO2e', 'kWh', 'therms', 'gal', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnVh72pPYNVC"
   },
   "outputs": [],
   "source": [
    "# Конвертируем выбранные числовые столбцы в тип float (например воспользовавшись генератором и функцией any)\n",
    "\n",
    "for col in list(data.columns):\n",
    "    if ...:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkyxxAxA6M7m"
   },
   "outputs": [],
   "source": [
    "# Посчитаем сколько после всех манипуляций у нас стало пропущенных значений\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZPneW5C-6zz"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на статистики над колонками датасета при помощи метода describe\n",
    "# Какие значения они принимают?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVhMD6r1CNmc"
   },
   "outputs": [],
   "source": [
    "# Какой размер у матрицы, которую возвращает метод describe?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1HBnkcpB2lZ"
   },
   "source": [
    "##### Вопрос:\n",
    "Почему вывелась статистика не по всем столбцам (всего их 60)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ork7-xDC-6z0"
   },
   "source": [
    "# Пропущенные данные и выбросы\n",
    "\n",
    "В добавок к некорректному определению типов данных, другая частая проблема — это пропуски в данных. У наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Для начала попробуем оценить масштаб проблемы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYFuKM53-6z0"
   },
   "outputs": [],
   "source": [
    "# Воспользуемся следующей функцией, которая предоставляет отчет о пропусках в данных\n",
    "\n",
    "def missing_values_table(df):\n",
    "    \"\"\"\n",
    "    Функция возвращает резюме по пропущенным значениям\n",
    "    \"\"\"\n",
    "    # Общее число пропусков\n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    # Процент пропусков\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    \n",
    "    # Создадит таблицу с результатом\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    # Переименнуем колонки\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    \n",
    "    # Отсортируем по проценту пропущенных значений\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(1)\n",
    "    \n",
    "    # Выведем некоторую информацию\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "            \" columns that have missing values.\")\n",
    "    \n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx2xw8aS-6z0"
   },
   "outputs": [],
   "source": [
    "# Воспользуемся описанной выше функцией на нашем датасете\n",
    "\n",
    "missing_values_table(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xin1DT8-6z1"
   },
   "source": [
    "При удалении данных следует быть осторожным, тем не менее, признак нам вряд ли вообще пригодится, если пропусков в нем слишком много. В данном случае удаляем признаки, в которых пропусков больше 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu86r83Y-6z1"
   },
   "outputs": [],
   "source": [
    "# Получим отчет из функции missing_values_table\n",
    "missing_df = ...\n",
    "\n",
    "# Выберем мне колонки, дял которых '% of Total Values' больше 50\n",
    "missing_columns = list(...)\n",
    "\n",
    "print('We will remove %d columns.' % len(missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEN1Ts2d-6z1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Удалим столбцы с большим числом пропусков\n",
    "\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPMuxN67865X"
   },
   "outputs": [],
   "source": [
    "# Проверим, что датасет (в частности его размер) изменился ожидаемым образом\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn0KDPx4DdgW"
   },
   "outputs": [],
   "source": [
    "# Посмотрим еще раз на пропуски при помощи функции missing_values_table\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPI3xSdL-6z2"
   },
   "source": [
    "Оставшиеся пропуски необходимо заполнить, опираясь на одну из стратегий по заполнению пропусков, которые мы обсуждали на лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJcriLoHoyPi"
   },
   "source": [
    "### Заполнение пропусков\n",
    "\n",
    "Основная функция, которая используется для заполнения пропусов -- это функиция .fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omLgD9DZowEf"
   },
   "outputs": [],
   "source": [
    "# Она может принимать как одно значение (например -1)\n",
    "\n",
    "data.fillna(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEQCSrOeo6DU"
   },
   "outputs": [],
   "source": [
    "# Там и pd.Series, у которой индексы являются названиями столбцов, а значения, это те значения, на которые мы хотим заменить пропуски\n",
    "# Заменим например максимальным значением в столбце, применив .max()\n",
    "\n",
    "data.fillna(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2ETPDcuo9Ib"
   },
   "source": [
    "Пропуски в числовых признаках мы заполним средним по колонке (.mean() -- выдаст средние только для числовых столбцов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8uONg0BowJ6"
   },
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czp5jCGApQke"
   },
   "source": [
    "В категориальных будем использовать просто -1 в качестве символа пропуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Llnu1cAWowMQ"
   },
   "outputs": [],
   "source": [
    "# Поскольку этот код выполняется после предыдущей ячейки, то нам достаточно сделать fillna(-1), потому что пропуски в числовых признаках уже заполнены\n",
    "\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRJrigYIQLt3"
   },
   "source": [
    "# Используем Pandas Profiling\n",
    "\n",
    "[`Pandas Profiling`](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/) - это библиотека для генерации интерактивных отчетов на основе пользовательских данных: можем увидеть распределение данных, типы, возможные проблемы. \n",
    "\n",
    "Библиотека очень проста в использовании: можем создать отчет и отправить его кому угодно!\n",
    "\n",
    "\n",
    "Разобраться во внутренностях можно через [чтение исходных текстов](https://github.com/pandas-profiling/pandas-profiling/blob/develop/src/pandas_profiling/visualisation/plot.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OafYfB2aQRlp"
   },
   "source": [
    "**Использование**\n",
    "\n",
    "Пользоваться библиотекой очень просто — надо всего лишь указать объект DataFrame для которого нужно выполнить исследование. Это осуществляется методом:\n",
    "\n",
    "`df.profile_report()`, где df - исследуемый DataFrame.\n",
    "\n",
    "Методу можно передать следующие параметры:  \n",
    "\n",
    "* `title` - название отчёта, \n",
    "* `pool_size (int)` - количество потоков для выполнения (по умолчанию = 0 - используются все потоки),\n",
    "* `progress_bar (bool)` - если True, то показывается прогресс бар,\n",
    "* `explorative (bool)` - если True, то выполняется более глубокий анализ (для текстов и файлов),\n",
    "* `minimal (bool)` - если True, то ресурсоёмкие вычисления не выполняются. Рекомендуется при работе с большими датасетами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EclwwwkCQTup"
   },
   "outputs": [],
   "source": [
    "# Установите библиотеку и перезапустите среду (в google colab она уже установлена)\n",
    "\n",
    "#!pip3 install pandas-profiling==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keVp4olSQYM7"
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdtvB8xaQYQS"
   },
   "outputs": [],
   "source": [
    "profile_final = ProfileReport(data, title=\"Energy and water consumption\", explorative=True, minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv72czzqy5wn"
   },
   "outputs": [],
   "source": [
    "profile_final.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOf98cH0boU7"
   },
   "source": [
    "Это не единственное, что может данная библиотека. Посмотрите примеры и почитайте документацию и может быть сможете использовать pandas_profiling - отчет при исследовании в групповых проектах!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MBAg5Wt-6z2"
   },
   "source": [
    "# Предварительный анализ данных\n",
    "Теперь когда утомительный,  но совершенно необходимый —  этап чистки данных закончен, можно углубляться в анализ. Предварительный анализ данных (Exploratory Data Analysis — EDA) — это процесс который можно продолжать до бесконечности, на этом этапе мы строим графики, ищем закономерности, аномалии или связи между признаками.\n",
    "\n",
    "В общем цель этого этапа понять что эти данные могут дать нам. Обычно процесс начинается с обзора всего набора, затем переходит к его специфическим подмножествам. Любые находки могут быть по-своему интересны, также они могут дать нам ценные подсказки, например, по поводу относительной значимости различных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_rKqVtt-6z2"
   },
   "source": [
    "### Графики от одной переменной ([Univariate analysis](https://en.wikipedia.org/wiki/Univariate_(statistics)) )\n",
    "Напомню, что цель — это предсказание значения целевого признака, рейтинга энергопотребления (переименуем его в score в нашем наборе), так что целесообразно для начала понять, какое эта величина имеет распределение. Посмотрим на него, построив гистограмму с matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq0n1j8lDpCy"
   },
   "outputs": [],
   "source": [
    "figsize(8, 8)\n",
    "\n",
    "# Для удобства переименнуем столбец 'ENERGY STAR Score' в 'score' при помощи метода rename\n",
    "data = ...\n",
    "\n",
    "# Построим гистограмму распределения столбца score, воспользовавшись функцией plt.hist, указав число бинов (например) 100\n",
    "...\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of Buildings')\n",
    "plt.title('Energy Star Score Distribution')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eREuvEu_-6z2"
   },
   "source": [
    "### Вывод:\n",
    "#### **Пока выглядит довольно подозрительно!**\n",
    "\n",
    "Интересующий нас рейтинг представляет собой перцентиль, так что ожидаемо было бы увидеть равномерное распределение, где каждому значению соответствует примерно одинаковое количество зданий. Хотя в нашем случае на лицо диспропорция, больше всего зданий имеют максимальное значение рейтинга — 100, либо минимальное — 1 (высокий рейтинг это хороший показатель).\n",
    "\n",
    "Обратимся к описанию признака и вспомним, что он основывается на “предоставляемых отчетах об энергопотреблении”. Это, возможно, кое-что объясняет. Просить владельцев зданий отчитаться об эффективности использования электроэнергии, это почти то же самое, что просить поставить студента оценку самому себе на экзамене. В результате мы получаем не самую объективную оценку эффективности использования электроэнергии в зданиях.\n",
    "\n",
    "Если бы время не было ничем ограничено, стоило бы выяснить, почему большинство зданий имеют слишком высокие или слишком низкие значения рейтинга. Для этого нужно отфильтровать записи по этим зданиям и посмотреть, что у них общего. В нашу задачу не входит изобретение метода новой оценки эффективности энергопотребления, так что лучше сфокусироваться на предсказании рейтинга с тем, что есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuM5C73O-6z3"
   },
   "outputs": [],
   "source": [
    "figsize(8, 8)\n",
    "\n",
    "# Построим гистограмму распределения поля Site EUI, воспользовавшись функцией plt.hist, указав число бинов (например) 200\n",
    "...\n",
    "\n",
    "# Что можно увидеть на ней?\n",
    "\n",
    "plt.xlabel('Site EUI')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Site EUI Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWhzWNh_-6z3"
   },
   "source": [
    "А здесь мы видим явный выброс!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2Xk6uDu-6z3"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на статистики этой же колонки, воспользовавшись методом describe\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPIhR6-J-6z4"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на топ 10 самых больших значений в этой колонке\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwcxFB9j-6z4"
   },
   "source": [
    "Посмотрим внимательнее на эти выбросы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MHXaIlO-6z4"
   },
   "outputs": [],
   "source": [
    "# Выведем строки, значения столбца Site EUI является наибольшим\n",
    "# Можно ли сделать какие-то выводы касательно этих зданий?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVDtKugMJmcD"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 7))\n",
    "\n",
    "# Воспользуемся boxplot-ом из библиотеки seaborn для того, чтобы убедиться, что эти объекты действительно похожы на выбросы\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5VSpJNHXOf8"
   },
   "source": [
    "---\n",
    "\n",
    "### Reminder: BoxPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRQg0DCeXAkE"
   },
   "source": [
    "<img src=https://i.ytimg.com/vi/BE8CVGJuftI/maxresdefault.jpg width=\"700\">\n",
    "\n",
    "Box plot --- график, использующийся в описательной статистике, компактно изображающий одномерное распределение вероятностей.\n",
    "\n",
    "Такой вид диаграммы в удобной форме показывает медиану (или, если нужно, среднее), нижний и верхний квартили, минимальное и максимальное значение выборки и выбросы. Несколько таких ящиков можно нарисовать бок о бок, чтобы визуально сравнивать одно распределение с другим; их можно располагать как горизонтально, так и вертикально. Расстояния между различными частями ящика позволяют определить степень разброса (дисперсии) и асимметрии данных и выявить выбросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHVLC447-6z5"
   },
   "source": [
    "---\n",
    "\n",
    "## Убираем выбосы ([Outliers](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm))\n",
    "\n",
    "Cnоит избавиться от выбросов. Они могут быть связаны с опечатками, ошибками в единицах измерения или являться корректными, но чересчур экстремальными значениями. Удалять выбросы можно разными способами, но один из самых популярных работает по следующей схеме:\n",
    "\n",
    "- Ниже `первой квартили - 3 $\\cdot$ (межквартильное расстояние)`\n",
    "- Выше `третьей квартили + 3 $\\cdot$ (межквартильное расстояние)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meLQqNw1-6z5"
   },
   "outputs": [],
   "source": [
    "# Рассчитаем первый и третий квантили поля Site EUI или возьмем значения из вывода метода describe\n",
    "first_quartile = ...\n",
    "third_quartile = ...\n",
    "\n",
    "# Рассчитаем IQR как разницу между третьим и первым квантилями\n",
    "iqr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyxzr1bqKdGQ"
   },
   "outputs": [],
   "source": [
    "# Создадим условие на столбец Site EUI для отбора НЕ выбросов, воспользовавшись выражением выше\n",
    "condition = ...\n",
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSpMwhEwKh3D"
   },
   "outputs": [],
   "source": [
    "# Применим созданное условие к нашим данным, отфильтровав тем самым строки, не являющиеся выбросами\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyL8r_WXKmqy"
   },
   "outputs": [],
   "source": [
    "# Посмотрим какого размера у нас стал датасет\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-wLV03hL96R"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на сами данные\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi1TAEz5-6z5"
   },
   "outputs": [],
   "source": [
    "figsize(8, 8)\n",
    "\n",
    "# Построим еще раз гистограмму распределения поля Site EUI, воспользовавшись функцией plt.hist, указав число бинов (например) 20\n",
    "...\n",
    "\n",
    "plt.xlabel('Site EUI')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Site EUI Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVq05zqiLu_B"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 7))\n",
    "\n",
    "# Еще раз Ввоспользуемся boxplot-ом из библиотеки seaborn для того, чтобы убедиться, что выбросы пропали\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANm44ob6a2oz"
   },
   "source": [
    "### Существует множество способов борьбы с выбросами\n",
    "\n",
    "Мы использовали этот:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tygwtFeaTSA"
   },
   "source": [
    "- **Dropping the outlier rows with standard deviation**\n",
    "```\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean() + data['column'].std() * factor\n",
    "lower_lim = data['column'].mean() - data['column'].std() * factor\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1nFbLYWa9ck"
   },
   "source": [
    "А могли бы и этот:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBg8AnPhatiD"
   },
   "source": [
    "- **Dropping the outlier rows with Percentiles**\n",
    "```\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nxVWQQMZejT"
   },
   "source": [
    "**Примеры других способов:**\n",
    "- Винсоризация — это серия трансформаций, направленных на ограничения влияния выбросов. 90%-ая винсоризация означает, что мы берём значения меньше 5% перцентиля и выше 95% перцентиля и приравниваем их к значениям на 5-м и 95-м перцентилях соответствиино.\n",
    "- Триминг - Триминг отличается от винсоризация тем, что мы не ограничиваем крайние значения каким-либо числом, а просто удаляем их.\n",
    "\n",
    "Больше методов представлено в [(scipy.stats.mstats)](https://docs.scipy.org/doc/scipy-0.14.0/reference/stats.mstats.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPCdWmmV-6z5"
   },
   "source": [
    "Возвращаемся назад к анализу:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLvxMqUT-6z5"
   },
   "source": [
    "# В поисках отношений\n",
    "Значительную часть работы на этапе EDA занимает поиск взаимосвязей между различными признаками. Очевидно что признаки и значения признаков, оказывающие основное влияние на целевой, интересуют нас сильнее, чем прочие, по ним лучше всего и предсказывать значение целевого. Одним из способов оценить влияние значений категориальных признаков (число значений такого признака подразумевается конечным) на целевой — **density plot**, например, используя модуль **seaborn**.\n",
    "\n",
    "**Density plot** можно представить себе как сглаженную гистограмму, потому что она показывает распределение одного значения категориально признака. Раскрасим распределения разными цветами и посмотрим на распределения. Код ниже строит density plot рейтинга энергопотребления. Разными цветами показаны рейтинги различных типов зданий (рассмотрены типы с как минимум сотней записей в нашем наборе):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MduVfQlc-6z5"
   },
   "outputs": [],
   "source": [
    "# Создадим список types с названиями категорий зданий с более, чем 100 измерениями\n",
    "types = data.dropna(subset=['score'])\n",
    "types = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi42y1op-6z5"
   },
   "outputs": [],
   "source": [
    "figsize(10, 6)\n",
    "\n",
    "# Постройте на одном графике распределения оценок для каждого типа зданий из списка types\n",
    "for b_type in types:\n",
    "    # Выберете данные одного из типов зданий из списка types\n",
    "    subset = ...\n",
    "    \n",
    "    # Воспользуйтесь функцией kdeplot библиотеки seaborn для отрисовки распределения\n",
    "    sns.kdeplot(..., label = ..., shade = False, alpha = 0.8)\n",
    "    \n",
    "plt.xlabel('Energy Star Score', size = 18)\n",
    "plt.ylabel('Density', size = 18)\n",
    "plt.title('Density Plot of Energy Star Scores by Building Type', size = 22)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yetl7zkv-6z6"
   },
   "source": [
    "Видно, что тип здания оказывает существенное влияние на рейтинг энергопотребления. Здания, используемые как офисы, чаще имеют хороший рейтинг, а отели наоборот. Получается, такой признак, как тип здания, для нас важен. Так как это признак категориальный, нам ещё предстоит выполнить с ним так называемый «one-hot encode».\n",
    "\n",
    "#### Повторим то же самое посмотрим для различных районов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jXcnItu-6z6"
   },
   "outputs": [],
   "source": [
    "# Создадим список boroughs с названиями районов с более, чем 100 измерениями\n",
    "boroughs = data.dropna(subset=['score'])\n",
    "boroughs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlEvD-gv-6z6"
   },
   "outputs": [],
   "source": [
    "figsize(12, 10)\n",
    "\n",
    "# Постройте на одном графике распределения оценок для каждого района из списка boroughs\n",
    "for borough in boroughs:\n",
    "    # Выберете данные одного из районов из списка boroughs\n",
    "    subset = ...\n",
    "    \n",
    "    # Воспользуйтесь функцией kdeplot библиотеки seaborn для отрисовки распределения\n",
    "    sns.kdeplot(..., label = ...)\n",
    "\n",
    "plt.xlabel('Energy Star Score', size = 18)\n",
    "plt.ylabel('Density', size = 18)\n",
    "plt.title('Density Plot of Energy Star Scores by Borough', size = 22)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6-FjmFS-6z6"
   },
   "source": [
    "Похоже, что район оказывает уже не такое большое влияние. Тем не менее, пожалуй, стоит включить этот признак в модель, так как определенная разница между районами все же есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1e_wj-p-6z6"
   },
   "source": [
    "## Корреляции между фичами и целевой переменной\n",
    "\n",
    "Чтобы численно оценить степень влияния признаков можно использовать [коэффициент корреляции Пирсона](http://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/). \n",
    "\n",
    "Это мера степени и положительности линейных связей между двумя переменными.  Значение в +1 означает идеальную пропорциональность между значениями признаков и, соответственно, в -1 аналогично, но с отрицательным коэффициентом.\n",
    "\n",
    "Несмотря на то что это не дает нам никакого понятия о непропорциональных взаимосвязях, это уже хорошее начало. В Pandas рассчитать величину корреляции довольно легко:\n",
    "\n",
    "```\n",
    "# Find all correlations with the score and sort\n",
    "correlations_data = data.corr()['score'].sort_values()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dRO9_nf-6z6"
   },
   "outputs": [],
   "source": [
    "# Воспользуемся методом corr(), выберем столбец с целевой меткой 'score' и отсортируем значения корреляции\n",
    "correlations_data = ...\n",
    "\n",
    "# Выведем 15 признаков с наименьшей прямой корреляцией \n",
    "print(...)\n",
    "\n",
    "print('------------')\n",
    "\n",
    "# Выведем 15 признаков с наибольшей прямой корреляцией \n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6twpo_D4-6z7"
   },
   "source": [
    "Можно видеть что есть несколько признаков имеющих высокие отрицательные значения коэффициента Пирсона, с самой большой корреляцией для разных категорий EUI (они между собой слегка отличаются по способу расчета). EUI — Energy Use Intensity — это количество использованной энергии, разделенное на площадь помещений в квадратных футах. Значит, чем этот признак ниже, тем лучше. Соответственно: с ростом EUI, рейтинг энергопотребления становится ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9I5liwbiZ5u"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# Воспользуемся функцией heatmap библиотеки seaborn для отрисовки матрицы корреляции (укажем параметр cbar=True)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83wni2-6z7"
   },
   "source": [
    "## Графики от двух переменных (bivariate analysis)\n",
    "Чтобы посмотреть на связь между двумя непрерывными переменными, можно использовать scatterplots (точечные графики). Дополнительную информацию, такую как значения категориальных признаков, можно показывать различными цветами. Например, график снизу показывает разброс рейтинга энергопотребления в зависимости от величины Site EUI, а разными цветами показаны типы зданий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnF0TghxXqk7"
   },
   "outputs": [],
   "source": [
    "# Сделаем копию нашего датасета для того, чтобы не изменить \n",
    "features = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIbf5_BT-6z8"
   },
   "outputs": [],
   "source": [
    "figsize(8, 6)\n",
    "\n",
    "# Выберем те строки поля с типами зданий, которым соответсвтуют известные значения рейтинга\n",
    "features['Largest Property Use Type'] = ...\n",
    "\n",
    "# Выберем только не строки, которые соответствуют типам зданий, содержащим более 100 записей (.isin(types))\n",
    "features = ...\n",
    "\n",
    "# Воспользуемся функцией lmplot библиотеки seaborn для построения графика scatterplot\n",
    "# По осям которого будут столбцы score и Site EUI, а точки делятся на группы по типам зданий\n",
    "sns.lmplot(..., ..., \n",
    "          hue = ..., data = ...,\n",
    "          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n",
    "          size = 10, aspect = 1.2)\n",
    "\n",
    "plt.xlabel('Site EUI', size = 18)\n",
    "plt.ylabel('Energy Star Score', size = 18)\n",
    "plt.title('Energy Star Score vs Site EUI', size = 22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k5w_lkD-6z8"
   },
   "source": [
    "Этот график наглядно демонстрирует, что такое коэффициент корреляции со значением -0.7. Site EUI уменьшается, и рейтинг энергопотребления уверенно возрастает, независимо от типа здания.\n",
    "\n",
    "## Pairs plot\n",
    "Ну и наконец, построим Pairs Plot. Это мощный исследовательский инструмент, он позволяет взглянуть на взаимосвязи сразу между несколькими признаками одновременно, а так же на их распределение. В примере при построении использовался модуль seaborn и функция PairGrid. Построен Pairs Plot со scatterplots выше главной диагонали, гистограммами на главной диагонали и 2D kernel density plots, с указанием корреляции, ниже главной диагонали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED0SdZTS-6z8"
   },
   "outputs": [],
   "source": [
    "# Выберем колонки, на которые мы будем смотреть в попарном разрезе\n",
    "plot_data = features[['score', 'Site EUI (kBtu/ft²)', \n",
    "                      'Weather Normalized Source EUI (kBtu/ft²)']]\n",
    "\n",
    "# Заменим бесконечности на np.nan при помощи функции replace\n",
    "plot_data = ...\n",
    "\n",
    "# Для удобства переименнуем колонки\n",
    "plot_data = plot_data.rename(columns = {'Site EUI (kBtu/ft²)': 'Site EUI', \n",
    "                                        'Weather Normalized Source EUI (kBtu/ft²)': 'Weather Norm EUI',\n",
    "                                        'log_Total GHG Emissions (Metric Tons CO2e)': 'log GHG Emissions'})\n",
    "\n",
    "# Удалим пропущенные значения\n",
    "plot_data = ...\n",
    "\n",
    "# Воспользуемся следующей функцией для подсчета корреляции между двумя векторами\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "\n",
    "# Создадим объект PairGrid и настроем в нем отображение графиков\n",
    "grid = sns.PairGrid(data = plot_data, size = 3)\n",
    "\n",
    "# Если интересно за что отвечает каждый метод, то можно их потыкать и посмотреть как меняется результат:)\n",
    "grid.map_upper(plt.scatter, color = 'red', alpha = 0.6)\n",
    "grid.map_diag(plt.hist, color = 'red', edgecolor = 'black')\n",
    "grid.map_lower(corr_func);\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.Reds)\n",
    "\n",
    "plt.suptitle('Pairs Plot of Energy Data', size = 36, y = 1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B17_rzbL-6z8"
   },
   "source": [
    "Чтобы посмотреть на интересующие нас отношения между величинами, ищем пересечения строк и колонок. Например, чтобы взглянуть на корреляцию между Weather Norm EUI со score, смотрим на строку Weather Norm EUI  и колонку score. Видно, что коэффициент Пирсона равен -0.67. Помимо того, что график красиво выглядит, он ещё может помочь понять, какие признаки стоит включить в нашу модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVKAOS0L-6z8"
   },
   "source": [
    "\n",
    "# Выбор и создание новых признаков\n",
    "Выбор и создание новых признаков зачастую оказывается одним из самых «благодарных» занятий по соотношению усилия/вклад в результат. Для начала, пожалуй, поясню что это такое:\n",
    "\n",
    "* __[Создание новых признаков, Feature Engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)__: процесс при котором берутся данные как они есть и затем на основе имеющихся данных конструируются новые признаки. Это может означать изменение непосредственно самих значений, например логарифмирование, взятие корня, или one—hot encoding категориальных признаков для того чтобы модель могла их эти признаки обработать. Иногда это создание совершенно новых признаков, которые раньше явным образом в данных не содержались, но, в общем, это всегда добавление в набор новых признаков, полученных из первоначальных данных.\n",
    "\n",
    "* __[Выбор признаков, Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)__: процесс выбора наиболее релевантных признаков. При этом из набора удаляются признаки для того чтобы модель уделила больше внимания и ресурсов первостепенным признакам, а также это помогает получить более легкоинтерпретируемые результаты. В общем, это чистка набора при которой остаются только наиболее важные для нашей задачи данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEbHR_02-6z8"
   },
   "source": [
    "В машинном обучении модель обучается целиком на данных которые мы подаем на вход модели, так что важно быть уверенным в том что все ключевые данные для эффективного решения задачи у нас есть. Если данных способных нам обеспечить решение задачи у нас нет, то какой бы модель не была хорошей, научить мы её ничему не сможем.\n",
    "\n",
    "На данном этапе я выполнил следующую последовательность действий:\n",
    "\n",
    "* One-hot кодирование категориальные признаков (район и тип здания);\n",
    "* Логарифмирование числовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpWivNXh-6z9"
   },
   "source": [
    "[One-hot кодирование](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) необходимо выполнить для того, чтобы модель могла учесть категориальные признаки. Модель не сможет понять, что имеется ввиду, когда указано, что здание используется как “офис”. Нужно создать новый соответствующий признак и присвоить ему значение 1, если данная запись содержит сведения об офисе и 0 в противном случае.\n",
    "\n",
    "При применении различных математических функций к значениям в наборе модель способна распознать не только линейные связи между признаками. [Взятие корня, логарифмирование, возведение в степень и т.д.](https://datascience.stackexchange.com/questions/21650/feature-transformation-on-input-data) — распространенная в науке о данных практика, и она может основываться на наших представлениях о поведении и связях между признаками, а так же просто на эмпирических сведениях о том, при каких условиях модель работает лучше. Тут я, как уже упоминалось, решил взять натуральный логарифм от всех числовых признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79d4YwWlQJ05"
   },
   "source": [
    "Приведенный ниже код этим и занимается: логарифмирует числовые признаки, а так выделяет два упомянутых категориальных признака и применяет к ним one-hot кодирование. Затем объединяет полученные при этом наборы. Звучит довольно утомительно, но Pandas позволяет это проделать относительно легко."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14If00gQ-6z9"
   },
   "outputs": [],
   "source": [
    "# Сделаем копию наших данных data\n",
    "features = ...\n",
    "\n",
    "# Выделим числовые признаки из датасета при помощи функции select_dtypes\n",
    "numeric_subset = ...\n",
    "\n",
    "# Добавим столбцы, получающися из колонок из numeric_subset путем взятия натурального логарифма\n",
    "# Назовем их 'log_<ИМЯ СТОЛБЦА>'\n",
    "for col in numeric_subset.columns:\n",
    "    if col == 'score':\n",
    "        # Пропустим целевой столбец\n",
    "        continue\n",
    "    else:\n",
    "        ...\n",
    "        \n",
    "# Выберем категориальные признаки ('Borough' и 'Largest Property Use Type')\n",
    "categorical_subset = ...\n",
    "\n",
    "# Воспользуемся функцией get_dummies библиотеки pandas для того, чтобы примернить OHE к категориальным колонкам\n",
    "categorical_subset = ...\n",
    "\n",
    "# Соединим числовые признаки и категориальные\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Посмотрим какого размера у нас получился датасет\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkd-rFTL-6z9"
   },
   "source": [
    "В итоге в нашем наборе теперь всё ещё 11,000 записей (зданий) и 110 колонок (признаков). Не все эти признаки одинаково важны для нашей задачи, так что перейдем к следующему шагу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQKdeTZhQaPO"
   },
   "source": [
    "# Выбор признаков\n",
    "Многие из 110 признаков для нашей модели избыточны, т.к. некоторые из них сильно коррелируют. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AART6Vec-6z9"
   },
   "source": [
    "## Удаляем коллинеарные признаки\n",
    "\n",
    "[Коллинеарные признаки](http://psychologicalstatistics.blogspot.com/2013/11/multicollinearity-and-collinearity-in.html) имеют высокое значение корреляции.\n",
    "Например, зависимость Site EUI от Weather Normalized Site EUI, которая имеют коэффициент корреляции 0.997."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCUC-Vyw-6z9"
   },
   "outputs": [],
   "source": [
    "plot_data = data[['Weather Normalized Site EUI (kBtu/ft²)', 'Site EUI (kBtu/ft²)']].dropna()\n",
    "\n",
    "plt.plot(plot_data['Site EUI (kBtu/ft²)'], plot_data['Weather Normalized Site EUI (kBtu/ft²)'], 'bo')\n",
    "plt.xlabel('Site EUI')\n",
    "plt.ylabel('Weather Norm EUI')\n",
    "plt.title('Weather Norm EUI vs Site EUI, R = %0.4f' % np.corrcoef(data[['Weather Normalized Site EUI (kBtu/ft²)', 'Site EUI (kBtu/ft²)']].dropna(), rowvar=False)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md9cYKqU-6z9"
   },
   "source": [
    "Признаки, которые сильно коррелированны, называют [коллинеарными](https://en.wikipedia.org/wiki/Multicollinearity), и достаточно оставить один из таких признаков, чтобы помочь алгоритму лучше обобщать и получать более интерпретируемые результаты на выходе (на всякий случай уточню, что речь идет о признаках коррелированных между собой, а не с целевым признаком, последние очень даже помогают нашему алгоритму.)\n",
    "\n",
    "Есть много способов поиска коллинеарных признаков, например, один из широко используемых — расчет коэффициента увеличения дисперсии. Сам я решил использовать так называемый thebcorrelation коэффициент. Один из двух признаков будет автоматически удален если коэффициент корреляции для этой пары выше 0.6.\n",
    "\n",
    "Выбор именно такого порога может показаться необоснованным, но он был установлен опытным путем, при решении этой конкретной задачи. Машинное обучение наука в значительной степени экспериментальная и зачастую сводится к произвольному поиску лучших параметров безо всякого обоснования.  В итоге оставим всего  64 признака и один целевой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWsvA8z4gI-v"
   },
   "outputs": [],
   "source": [
    "# Отрисуем матрицу корреляции для наших новых признаков при помощи heatmap\n",
    "\n",
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1P3Q6Pf-6z-"
   },
   "outputs": [],
   "source": [
    "# Воспользуемся следующей функцией для того, что избавить от сильно скоррелированных признаков\n",
    "\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Удалит коллинеарные признаки в dataframe с коэффициентом корреляции, превышающим пороговое значение.\n",
    "        Удаление коллинеарных функций может помочь модели обобщить и улучшить интерпретируемость модели,\n",
    "        а также позвоялет избавить от бесполезных с точки зрения информации признаков\n",
    "        \n",
    "    Inputs: \n",
    "        x: исходная матрица признаков\n",
    "        threshold: любые объекты с корреляциями выше этого значения удаляются\n",
    "    \n",
    "    Output: \n",
    "        dataframe который содержит только слабо коллинеарные признаки\n",
    "    '''\n",
    "    \n",
    "    # Не хотим удалять корреляцию между Energy Star Score\n",
    "    y = x['score']\n",
    "    x = x.drop(columns = ['score'])\n",
    "    \n",
    "    # Вычисляем матрицу корреляции\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Проходим по всей матрице и сравниваем корреляции\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # Если корреляция превышает порог, то добавляем кандидата на удаление\n",
    "            if val >= threshold:\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Удаляем один из двух скоррелированных признака\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu/ft²)', \n",
    "                          'Water Use (All Water Sources) (kgal)',\n",
    "                          'log_Water Use (All Water Sources) (kgal)',\n",
    "                          'Largest Property Use Type - Gross Floor Area (ft²)'])\n",
    "    \n",
    "    x['score'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCS45yRM-6z-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Воспользуемся написанноый выше функцией, чтобы удалить сильно скоррелированные признаки\n",
    "features = remove_collinear_features(features, 0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0mJSrF-6z-"
   },
   "outputs": [],
   "source": [
    "# Удалим колокни, которые содержат в себе только пропуски (dropna(axis=1, how='all'))\n",
    "features = ...\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtgW-0rMbrEg"
   },
   "outputs": [],
   "source": [
    "# После всех проведенных нами махинаций отрисуем еще раз матрицу корреляции\n",
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2wT932w-6z-"
   },
   "source": [
    "#### Дополнительный Feature Selection\n",
    "\n",
    "Существует множество типов [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html). Самый популярный: [principal components analysis (PCA)](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) который преобразует функции в уменьшенное количество измерений, которые сохраняют наибольшую вариативность, или [independent components analysis (ICA)](http://cs229.stanford.edu/notes/cs229-notes11.pdf) который стремится найти независимые источники в наборе фичей. Хотя эти методы эффективны для уменьшения количества признаков, они создают новые признаки, которые не имеют физического смысла и, следовательно, делают интерпретацию модели практически невозможной.\n",
    "\n",
    "Эти методы очень полезны для работы с многомерными данными, и я советую к прочтению[ источник](https://machinelearningmastery.com/feature-selection-machine-learning-python/) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
